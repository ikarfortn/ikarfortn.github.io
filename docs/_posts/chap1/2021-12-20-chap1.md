---
layout: single
title:  "RL chapter 1"
date:   2021-12-20 
toc: true
permalink: /docs/chap1/
#url: http://127.0.0.1:4000/ikarfortn.github.io/docs/_posts/chap1/#chapter-1
# #url: ikarfortn/ikarfortn.github.io/docs/chap1
# url: docs/_site/2021/12/20
# #permalink: ikarfortn.github.io/docs/_posts/chap1/
# #url: /ikarfortn.github.io/docs/_site/ikarfortn.github.io/
---
  *resources to refer: Micah Carroll blog - very helpful* 

<mark></mark> 
**Chapter 1**

**taken from the book:**
The most important feature distinguishing reinforcement learning from other types of learning is that it <mark>uses training information that evaluates the actions taken rather than instructs by giving correct actions</mark>. This is what creates <mark>the need for active exploration</mark>, for an explicit search for good behavior. <mark>Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible.</mark> <mark>Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken.</mark> This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: <mark>evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken.</mark>

**notes :**  
*instructive feedback:*   
teacher writing the solution on the board and students copy the solution
evaluative feedback: the student tries his own ways to find the solution to a problem and the teacher guides him by scoring his solution. The student continually tries to maximize his score. He might stop when he reaches a satisfactory score and use this particular way again and again (exploit) or might continue trying to find a further better solution (explore)

*layman rough RL:*   
Agent lands in a foreign uncertain environmennt, tries out different things to achieve a goal X, the more it tries, better it learns to navigate this environment. It uses the tools that it has found to try hitting the goal and also searches for new tools to crack the goal better. 

non associative problems are problems in which an action taken at time t, will not affect the possiblities of the agent in the future. 

<mark>In RL the agent generates its own training data by interaction with its environment</mark>

## K armed bandit problem 

Q star is the mean of the distribution for each action.  
refresher video: [https://notability.com/n/FT6HDReG4NAKvaiH~nCjN]

The expected reward is q star a.

![](/images/equation.png)
 <!-- insert k armed video here  -->

**stochastic tasks:** each action must be tried several times to gain reliable estimate of expected reward as estimate of expected reward as reward itself has a bit of randomness of it. 
 

<html>
  <head>
    <script type="module" src="https://js.withorbit.com/orbit-web-component.js"></script>
  </head>
  <body>
    <orbit-reviewarea color="pink">
      <orbit-prompt
        question="RL uses training information to do what and does not do what?"
        answer="to evaluate actions not instruct by giving correct actions "
      ></orbit-prompt>
      <orbit-prompt
        question="Purely instructive feedback indicates how good the action taken was. But it does not indicate whether or not _____________"
        answer="it was the best course of action."
      ></orbit-prompt>
       <orbit-prompt
        question="What is q star t"
        answer="q star t equals expectation of reward r given action a is selected."
      ></orbit-prompt>
        <orbit-prompt
        question="when you press on the buttons do you get the same values"
        answer="not necessarily. the expected reward has randomness involved therefore we calculate the mean of the values we get when we press the buttons. 
        And this mean is 
        $$q_{*}(a)=\mathbb{E}\left[R_{t} \mid A_{t}=a\right]$$. 
        the approximation of this mean is $$Q_{t}(a)$$"
      ></orbit-prompt>
    </orbit-reviewarea>
  </body>
</html>

## action value method 

true val of the action (q star t) is the mean reward when the action is selected 

Q(t) is the estimate of q star t
as you increase the number of samples you take Q(a) will converge to q_star_t







## how greedy?

<mark>as the variance of the rewards increase 
it is better to be more greedy</mark>. it would take more exploration to find optimal action.

if the rewards are not noisy, greedier approaches work better because the greedy method would <mark> know the true action value after just trying once.</mark>

but still, this applies only for <mark>stationary</mark> probabilities. in this case <mark>exploration is needed even in deterministic scenarios to check if non- greedy actions have become better than the greedy one.</mark>

In exploration, 
  - if epsilon is 0 then no exploration occurs
  - if epsilon is bigger than necessary, it finds the optimum action early on, but still doesn't use that (is not greedy) and doesn't select the optimum action enough
  - if epsilon is smaller than necessary, it takes a longer time to find the optimum action, but uses it more. 

## how greedy?
### incremental implementation:
store the last calculated average and incrementally modify the last estimate. 

say you have the avg marks of a class of some 59 students. Let's say this equals 65. now there is a new kid and his marks are say 80. Now imagine a plot of the marks with a mean of 65.
now to include the new students mark you can do either of these:
- you can plot the new 60th point, calculate a new line of best fit.
- or you can tweak all the old 59 points slightly such that the line of best fit obtained for the tweak points would match the lobf obtained by the first idea. such a tweak would mean shifting each of the points by (avg of old graph - new data point) / 59. 

the core idea being that 
say you weigh 4 chocolate brownie balls and calcualte their avg
and you add a new ball and calculate the new avg 
it is essentially the same as disintegrating the new ball 
adding a little bit of the new ball to the older balls 
and weighing the avg. 
![](/images/ball.png)




<html>
  <head>
    <script type="module" src="https://js.withorbit.com/orbit-web-component.js"></script>
  </head>
  <body>
    <orbit-reviewarea color="yellow">
      <orbit-prompt
        question="[target-old estimate] is usually considered as"
        answer="error in the estimate"
      ></orbit-prompt>
      <orbit-prompt
        question="state the formula by which you incrementally compute new reward estimate"
        answer="old estimate+ stepsize[target-old estimate]"
      ></orbit-prompt>
    </orbit-reviewarea>
  </body>
</html>