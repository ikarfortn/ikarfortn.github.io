---
layout: single
title:  "RL chapter 1"
date:   2021-12-20 
toc: true
permalink: /docs/chap1/
#url: http://127.0.0.1:4000/ikarfortn.github.io/docs/_posts/chap1/#chapter-1
# #url: ikarfortn/ikarfortn.github.io/docs/chap1
# url: docs/_site/2021/12/20
# #permalink: ikarfortn.github.io/docs/_posts/chap1/
# #url: /ikarfortn.github.io/docs/_site/ikarfortn.github.io/
---
  *resources to refer: Micah Carroll blog - very helpful* 

<mark></mark> 
**Chapter 1**

The most important feature distinguishing reinforcement learning from other types of learning is that it <mark>uses training information that evaluates the actions taken rather than instructs by giving correct actions</mark>. This is what creates <mark>the need for active exploration</mark>, for an explicit search for good behavior. <mark>Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible.</mark> <mark>Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken.</mark> This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: <mark>evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken.</mark>

instructive feedback: teacher writing the solution on the board and students copy the solution
evaluative feedback: the student tries his own ways to find the solution to a problem and the teacher guides him by scoring his solution. The student continually tries to maximize his score. He might stop when he reaches a satisfactory score and use this particular way again and again (exploit) or might continue trying to find a further better solution (explore)

layman rough RL: Agent lands in a foreign uncertain environmennt, tries out different things to achieve a goal X, the more it tries, better it learns to navigate this environment. It uses the tools that it has found to try hitting the goal and also searches for new tools to crack the goal better. 

non associative problems are problems in which an action taken at time t, will not affect the possiblities of the agent in the future. 

<mark>In RL the agent generates its own training data by interaction with its environment</mark>

## K armed bandit problem 



 <!-- insert k armed video here  -->


 

<html>
  <head>
    <script type="module" src="https://js.withorbit.com/orbit-web-component.js"></script>
  </head>
  <body>
    <orbit-reviewarea color="pink">
      <orbit-prompt
        question="RL uses training information to what and does not do what?"
        answer="to evaluate actions not instruct by giving correct actions "
      ></orbit-prompt>
      <orbit-prompt
        question="Purely instructive feedback indicates how good the action taken was. But it does not indicate whether or not _____________"
        answer="it was the best course of action."
      ></orbit-prompt>
    </orbit-reviewarea>
  </body>
</html>

