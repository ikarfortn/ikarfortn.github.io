---
layout: single
title:  "RL chapter 1"
date:   2021-12-20 
toc: true
permalink: /docs/chap1/
#url: http://127.0.0.1:4000/ikarfortn.github.io/docs/_posts/chap1/#chapter-1
# #url: ikarfortn/ikarfortn.github.io/docs/chap1
# url: docs/_site/2021/12/20
# #permalink: ikarfortn.github.io/docs/_posts/chap1/
# #url: /ikarfortn.github.io/docs/_site/ikarfortn.github.io/
---

<mark></mark> 
 **Chapter 1**

 The most important feature distinguishing reinforcement learning from other types of learning is that it <mark>uses training information that evaluates the actions taken rather than instructs by giving correct actions</mark>. This is what creates <mark>the need for active exploration</mark>, for an explicit search for good behavior. <mark>Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible.</mark> <mark>Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken.</mark> This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: <mark>evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken.</mark>

 In this chapter we study the evaluative aspect of reinforcement learning 
 in a simplified setting, one that does not involve learning to act in more than one situation. This nonassociative setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback di↵ers from, and yet can be combined with, instructive feedback. 

 ## K armed bandit problem 
 You are faced repeatedly with a choice among k di↵erent options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.

Each action selection is like a play of one of the slot machine’s levers, and the rewards are the payo↵s for hitting the jackpot. Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers.

In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q*(a), is the expected reward given that a is selected:

q⇤(a)=. E[Rt|At=a].



 <!-- insert k armed video here  -->


 

<html>
  <head>
    <script type="module" src="https://js.withorbit.com/orbit-web-component.js"></script>
  </head>
  <body>
    <orbit-reviewarea color="pink">
      <orbit-prompt
        question="RL uses training information to what and does not do what?"
        answer="to evaluate actions not instruct by giving correct actions "
      ></orbit-prompt>
      <orbit-prompt
        question="Purely instructive feedback indicates how good the action taken was. But it does not indicate whether or not _____________"
        answer="it was the best course of action."
        <!-- meme: is a sol. is it the best solution tho -->
      ></orbit-prompt>
      <orbit-prompt
        question="Given a right triangle with legs of length $a$ and $b$, what is the length of hypotenuse $c$?"
        answer="$$c = \sqrt{a^2 + b^2}$$"
      ></orbit-prompt>
    </orbit-reviewarea>
  </body>
</html>

 <!-- question-attachments="https://docs.withorbit.com/toffoli.png" -->
  <!-- question-attachments= add img of teacher giving sol vs student thinking  -->